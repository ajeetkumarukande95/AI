{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science\n",
        "\n",
        "### Definition:\n",
        "Data Science is the process of extracting meaningful knowledge, insights, and information from data using scientific methods and resources.\n",
        "\n",
        "### Scientific Methods/Resources:\n",
        "1. **Machine Learning**: Analyzing structured data like CSV, Excel, MongoDB, SQLite.\n",
        "2. **Deep Learning**: Working with unstructured data such as images.\n",
        "3. **Natural Language Processing (NLP)**: Handling text data.\n",
        "4. **Statistics**: Applying statistical techniques to analyze data.\n",
        "5. **Data Visualization**: Tools like Seaborn, Matplotlib, and Pandas to create visual representations of data.\n",
        "\n",
        "### Types of Data:\n",
        "1. **Structured Data**: Data in a defined format, e.g., CSV, Excel.\n",
        "2. **Unstructured Data**: Data without a predefined format, e.g., images, videos, text, audio, PDF.\n",
        "3. **Semi-Structured Data**: Data with some structure, e.g., JSON, HTML.\n",
        "\n",
        "---\n",
        "\n",
        "# Machine Learning\n",
        "\n",
        "### Definition:\n",
        "Machine Learning (ML) is a subset of AI and a branch of computer science focused on programming systems to automate learning from past data/experiences. ML models improve performance and make predictions based on training data.\n",
        "\n",
        "### Training and Testing Data:\n",
        "- **Training Data**: Used to train the model (80% of total data).\n",
        "  - Example: If the total dataset has 600 samples, the training set will consist of 480 samples.\n",
        "- **Testing Data**: Unseen data used to test the model's accuracy (20% of total data).\n",
        "  - Example: For a dataset of 600 samples, the testing set will consist of 120 samples.\n",
        "\n",
        "---\n",
        "\n",
        "### Real-World Applications of Machine Learning:\n",
        "1. **Medical Field**: Disease detection (e.g., Cancer, Covid, Diabetes).\n",
        "2. **House Price Prediction**: Forecasting real estate prices.\n",
        "3. **Stock Price Prediction**: Analyzing market trends.\n",
        "4. **Financial Domain**: Approvals or risk analysis (e.g., high risk, medium, low).\n",
        "5. **Handwritten Character Recognition**: Identifying characters in scanned documents.\n",
        "6. **Sentiment Analysis**: Classifying sentiments (e.g., Happy/Sad, Positive/Negative, Spam/Not Spam).\n",
        "7. **ChatBots**: E-commerce bots like Amazon and Flipkart assistants.\n",
        "8. **Weather Prediction**: Forecasting future weather conditions.\n",
        "9. **Speech Recognition**: Converting spoken words into text.\n",
        "10. **Advertisement**: Targeting users with personalized ads.\n",
        "11. **Self-Driving Cars**: Object detection and autonomous navigation.\n",
        "12. **Insurance**: Risk assessment for policies.\n",
        "\n",
        "---\n",
        "\n",
        "### Example - Health Insurance:\n",
        "- **Cancer Insurance**: Coverage up to 5 lakh.\n",
        "- **Diabetes Insurance**: Coverage up to 10 lakh.\n"
      ],
      "metadata": {
        "id": "CHHkmrXHhN8L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQHoSOVfhHdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "## What is Linear Regression?\n",
        "- **Linear**: Refers to a straight line or path.\n",
        "- **Regression**: Refers to predicting a continuous value or a real number.\n",
        "\n",
        "Linear regression is a predictive model used to find the linear relationship between a dependent variable and one or more independent variables.\n",
        "\n",
        "## Types of Linear Regression\n",
        "\n",
        "### 1. Simple Linear Regression\n",
        "   - Involves only **one independent variable**.\n",
        "   \n",
        "### 2. Multiple Linear Regression\n",
        "   - Involves **two or more independent variables**.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- The primary goal of linear regression is to find the **Best Fit Line**, which is also known as the **Regression Line**.\n",
        "\n",
        "### Linear Model Representation\n",
        "- For a simple linear regression:\n",
        "  \\[\n",
        "  y = mx + c\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( y \\) = Dependent variable\n",
        "  - \\( x \\) = Independent variable\n",
        "  - \\( m \\) = Slope of the line\n",
        "  - \\( c \\) = Intercept on the y-axis\n",
        "\n",
        "- For multiple linear regression with multiple independent variables:\n",
        "  \\[\n",
        "  y = m_1x_1 + m_2x_2 + m_3x_3 + \\dots + m_Nx_N + c\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( y \\) = Dependent variable\n",
        "  - \\( x_1, x_2,.... x_N \\) = Independent variables\n",
        "  - \\( m_1, m_2,... m_N \\) = Coefficients (slopes) of the independent variables\n",
        "  - \\( c \\) = Intercept\n",
        "\n",
        "## Variables:\n",
        "- **Dependent Variable** (\\( y \\)): A continuous or numeric variable that we aim to predict.\n",
        "- **Independent Variables** (\\( x \\)): These can be continuous/numeric or discrete variables used to predict the dependent variable.\n",
        "\n",
        "## Objective:\n",
        "The aim of linear regression is to predict the dependent variable \\( y \\) based on the independent variables \\( x \\).\n",
        "\n"
      ],
      "metadata": {
        "id": "-yoAYFkUxGdT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9M36cyShHgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression - Detailed Explanation\n",
        "\n",
        "## When \\( y = x \\)\n",
        "\n",
        "- **When the value of \\( x \\) is the same as \\( y \\):**\n",
        "  - \\( y = x \\)\n",
        "  - The equation becomes:\n",
        "    \\[\n",
        "    y = 1 \\times x\n",
        "    \\]\n",
        "  - This results in a line with a slope of 1, i.e., a **45-degree line**.\n",
        "  - The tangent of 45° is 1, so:\n",
        "    \\[\n",
        "    \\tan(45^\\circ) = 1\n",
        "    \\]\n",
        "  - Therefore, the equation becomes:\n",
        "    \\[\n",
        "    y = x\n",
        "    \\]\n",
        "\n",
        "## When \\( y \\neq x \\)\n",
        "\n",
        "- **Example 1**: When \\( x = 1 \\), \\( y = 2 \\); when \\( x = 2 \\), \\( y = 3 \\).\n",
        "- **Example 2**: When \\( x = 0 \\), what is the value of \\( y \\)? The value of \\( y \\) when \\( x = 0 \\) is the **intercept**.\n",
        "\n",
        "Thus, the general equation becomes:\n",
        "\\[\n",
        "y = mx + c\n",
        "\\]\n",
        "Where:\n",
        "- \\( m \\) is the slope, and\n",
        "- \\( c \\) is the intercept.\n",
        "\n",
        "- \\( m \\) and \\( c \\) reflect the relationship between \\( x \\) and \\( y \\) in your dataset.\n",
        "- The combination of \\( m \\) and \\( c \\) forms the **Best Fit Line (BFL)** or the **model**.\n",
        "\n",
        "### Example:\n",
        "- When \\( 0 = 25 \\), the tangent of \\( \\theta(0) \\) is calculated.\n",
        "  - **Q**: What happens when we change 1 unit in \\( x \\)? How does \\( y \\) change?\n",
        "  - **Multiple features**: Features represent dimensions in the feature space.\n",
        "  - **Feature space**: All dimensions of data in the model.\n",
        "\n",
        "## When we have multiple independent variables:\n",
        "\n",
        "- For two independent variables:\n",
        "  \\[\n",
        "  y = m_1x_1 + m_2x_2 + c\n",
        "  \\]\n",
        "\n",
        "### Feature Space & Model Dimensions:\n",
        "- **Feature space**: All dimensions of the data.\n",
        "- **Model dimensions**: One less than the feature space (since we exclude the dependent variable).\n",
        "\n",
        "## Hyperplanes in Multi-Dimensional Space:\n",
        "- **Hyperplanes**: When there are more than 4 dimensions, we cannot visualize them, but they possess all the properties of the feature space.\n",
        "- Linear models work in up to 300 dimensions, whereas the human brain is limited to 3 dimensions.\n",
        "- All 300 dimensions are orthogonal to each other.\n",
        "\n",
        "### Assumption of Independence:\n",
        "- The algorithm assumes that independent variables are independent of each other and do not influence each other.\n",
        "  \n",
        "### Example:\n",
        "- If we want to predict the weight of a car (dependent variable) using horse power (independent variable), in reality, **weight and horse power are related**.\n",
        "  - More weight generally means more horse power.\n",
        "  - However, the algorithm assumes that weight and horse power are independent of each other, which may create problems due to **collinearity**.\n",
        "\n",
        "### Collinearity Problem:\n",
        "- If there is strong collinearity (relationship between variables), we need to resolve it.\n",
        "- **Solution**: Dimensionality reduction, such as **Principal Component Analysis (PCA)**, which helps drop one of the correlated dimensions.\n",
        "\n",
        "## Correlation:\n",
        "\n",
        "### What is Correlation?\n",
        "- Correlation indicates how closely the relationship between two variables is.\n",
        "- We typically use **Pearson’s correlation**, which ranges from **-1 to +1**.\n",
        "  \n",
        "### Types of Correlation:\n",
        "- **Positive Correlation**: As \\( x \\) increases, \\( y \\) also increases (closer to +1).\n",
        "  - **Good predictor** when the correlation value is closer to +1.\n",
        "  \n",
        "- **Negative Correlation**: As \\( y \\) decreases, \\( x \\) increases (closer to -1).\n",
        "  - **Good predictor** when the correlation value is closer to -1.\n",
        "\n",
        "- **No Linear Relationship**: When the correlation is near **0**, it indicates no linear relationship between the variables, making it a **bad predictor**.\n",
        "\n",
        "### Coefficient of Correlation (R):\n",
        "- **R** is an indication of the strength of the relationship between two variables.\n",
        "- **Ideal relationship**: When \\( R = 1 \\) or \\( R = -1 \\), the regression line will pass through all points (this situation is rare).\n",
        "  \n",
        "### R Value Ranges:\n",
        "- **Good Predictors**: \\( R \\) values between **+0.7 to +1** and **-0.7 to -1**.\n",
        "- **Bad Predictors**: \\( R \\) values between **-0.3 to +0.3**.\n",
        "  \n",
        "### In-depth Study of Features:\n",
        "- **Features with \\( R \\) values between -0.7 and -0.3 or between 0.3 and 0.7** need to be studied more carefully.\n"
      ],
      "metadata": {
        "id": "C0kWPcgfz7mE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQRYDe4WhHkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 2"
      ],
      "metadata": {
        "id": "9Ek7udNo0XSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression - Correlation and Statistical Concepts\n",
        "\n",
        "## Correlation and R Value:\n",
        "- **R value near 0**: When data points are scattered, there is no clear linear relationship.\n",
        "- **R value near +1**: When \\( x \\) and \\( y \\) show a **positive linear relation**.\n",
        "- **R value near -1**: When \\( x \\) and \\( y \\) show a **negative linear relation**.\n",
        "\n",
        "## Computation of Correlation\n",
        "\n",
        "### Variance:\n",
        "- Variance measures the **variability** from the mean.\n",
        "  \\[\n",
        "  V = \\frac{\\sum (X_i - \\hat{X})^2}{n}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( X_i \\) is each individual data point,\n",
        "  - \\( \\hat{X} \\) is the mean of \\( X \\),\n",
        "  - \\( n \\) is the number of data points.\n",
        "\n",
        "### Central Tendency:\n",
        "- **Central Tendency**: The statistical measure that identifies a single value as a representation of an entire distribution.\n",
        "- It aims to provide an **accurate description** of the entire data.\n",
        "  - Measures of central tendency: **mean**, **mode**, and **median**.\n",
        "  \n",
        "- Variance provides the **reliability** of the central value.\n",
        "\n",
        "### Covariance:\n",
        "- Covariance measures how two features **vary together** or **influence each other**.\n",
        "  \\[\n",
        "  \\text{Cov} = \\frac{\\sum (X_i - \\hat{X})(Y_i - \\hat{Y})}{n}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( X_i \\) and \\( Y_i \\) are the individual data points for variables \\( X \\) and \\( Y \\),\n",
        "  - \\( \\hat{X} \\) and \\( \\hat{Y} \\) are the means of \\( X \\) and \\( Y \\),\n",
        "  - \\( n \\) is the number of data points.\n",
        "\n",
        "- **If covariance is 0**, the correlation will also be 0.\n",
        "\n",
        "### Standard Deviation:\n",
        "- Standard deviation measures the spread of data points from the mean.\n",
        "  \\[\n",
        "  \\text{std} = \\sqrt{\\frac{\\sum (X_i - \\hat{X})^2}{n}}\n",
        "  \\]\n",
        "\n",
        "### Pearson’s Correlation Coefficient:\n",
        "- The **Pearson correlation coefficient (R)** is computed as:\n",
        "  \\[\n",
        "  R(x, y) = \\frac{\\sum (X_i - \\hat{X})(Y_i - \\hat{Y})}{\\sqrt{\\sum (X_i - \\hat{X})^2} \\cdot \\sqrt{\\sum (Y_i - \\hat{Y})^2}}\n",
        "  \\]\n",
        "\n",
        "### Linear Regression Model:\n",
        "- When we build a **linear regression model**, the **Best Fit Line (BFL)** will always pass through the point where the **mean of \\( x \\)** and the **mean of \\( y \\)** meet.\n",
        "\n",
        "### How R Value Approaches 0:\n",
        "- When we compute \\( (X_i - \\hat{X}) \\) in the quadrant, we get a **symmetric quadrant distribution**.\n",
        "- When we sum all these values, positive and negative values cancel each other out, making the R value approach **0**.\n",
        "\n",
        "### How R Value Approaches +1:\n",
        "- When we compute \\( (X_i - \\hat{X}) \\) in the quadrant, we get an **asymmetric quadrant distribution**.\n",
        "- When we sum all these values, the positive and negative values do not cancel out and the R value approaches **+1**.\n",
        "\n",
        "### How R Value Approaches -1:\n",
        "- Similar to the above, when we compute \\( (X_i - \\hat{X}) \\) in the quadrant, we get an **asymmetric quadrant distribution**.\n",
        "- When we sum the values, the positive and negative values do not cancel out and the R value approaches **-1**.\n",
        "\n",
        "## Statistical Fluke:\n",
        "- **Statistical Fluke**: When \\( R \\) is between **0.5 to 0.7**, both positive and negative correlations may appear, which requires **further research**.\n",
        "- This happens due to **random sampling errors** while computing correlation, and it can be a **statistical fluke**.\n",
        "- A statistical fluke occurs because of problems in the **random sampling** process of the model.\n"
      ],
      "metadata": {
        "id": "IByao4u_CPj-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNYrSbsQhHm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lHMGgiSShHpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Iris.csv')"
      ],
      "metadata": {
        "id": "01FfA5VFhHsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataset\n",
        "print(\"Dataset Preview:\")\n",
        "print(df.head())  # Display the first few rows"
      ],
      "metadata": {
        "id": "U_8-PgQWhHzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check the number of unique values in each column\n",
        "print(\"\\nNumber of unique values in each column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()}\")"
      ],
      "metadata": {
        "id": "d7gNJ4SlhH5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Id' column as it is not relevant for analysis\n",
        "df.drop('Id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ON4DfW5LhH8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the modified dataset\n",
        "print(\"\\nModified Dataset after dropping 'Id' column:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "ovyWPL9whH-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "rB2TEQrYhIAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of each species\n",
        "species_counts = df['Species'].value_counts()\n",
        "print(\"\\nSpecies Counts:\")\n",
        "print(species_counts)"
      ],
      "metadata": {
        "id": "m7Ozen68hICn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace species names with numerical values for further analysis\n",
        "species_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
        "df['Species'].replace(species_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "_moqyxRLhIEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataset after mapping species\n",
        "print(\"\\nDataset after encoding 'Species':\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "p_2KfmgShIG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix again after species encoding\n",
        "correlation_matrix_updated = df.corr()\n",
        "print(\"\\nUpdated Correlation Matrix:\")\n",
        "print(correlation_matrix_updated)"
      ],
      "metadata": {
        "id": "aHA_zYtihIJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot pairwise relationships\n",
        "sns.pairplot(df, diag_kind='kde', hue='Species', palette='Set2')  # Add KDE plots on the diagonal\n",
        "plt.suptitle(\"Pairplot of Iris Dataset\", y=1.02)  # Add a title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EBdcC4HnhIxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "sns.heatmap(correlation_matrix_updated, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Heatmap of Correlation Matrix\")  # Add a title\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d8mnKzn6hI0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGPHDDeKhI69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Algorithm\n",
        "\n",
        "## 1. Best Fit Line (BFL)\n",
        "The **Best Fit Line (BFL)** is a straight line that minimizes the overall error between actual data points and predicted points.  \n",
        "It has the following properties:\n",
        "- Passes or goes through the **maximum number of data points**.\n",
        "- **Cannot pass through all data points**.\n",
        "- Minimizes the **distance between other points and the line**.\n",
        "- Ensures the **least sum of errors**.\n",
        "- Always passes through the point where **mean of \\(x\\) (\\(\\bar{x}\\))** and **mean of \\(y\\) (\\(\\bar{y}\\))** intersect.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Error/Residual\n",
        "- **Residual/Error**: The difference between the actual value and the predicted value:  \n",
        "  \\[\n",
        "  \\text{Error (Residual)} = Y_a - Y_p\n",
        "  \\]  \n",
        "  Where:  \n",
        "  - \\(Y_a\\): Actual value  \n",
        "  - \\(Y_p\\): Predicted value  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Concepts\n",
        "\n",
        "### Finding the Best Fit Line\n",
        "- The algorithm (linear model) identifies **one best fit line** from an infinite number of possibilities.\n",
        "- To accomplish this, the linear model uses a process called **Gradient Descent**.\n",
        "\n",
        "### Sum of Squared Errors (SSE) or Residual Sum of Squares (RSS)\n",
        "- Formula:  \n",
        "  \\[\n",
        "  SSE = \\frac{\\sum{(Y_a - Y_p)^2}}{n}\n",
        "  \\]  \n",
        "  Where:  \n",
        "  - \\(Y_a\\): Actual values  \n",
        "  - \\(Y_p\\): Predicted values  \n",
        "  - \\(n\\): Number of data points  \n",
        "\n",
        "- A **smaller error** indicates a more reliable model or central value.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Quadratic Equation and Convex Function\n",
        "- When we use the squared term, the equation forms a **quadratic equation**.\n",
        "- When we plot the values of **m (slope)** and **c (intercept)** against the error, the plot takes a **bowl shape**:\n",
        "  - This shape is called a **Convex Function**.\n",
        "  - A convex function guarantees one **absolute minimum (global minimum)**.\n",
        "- Each point on the \"bowl\" corresponds to a specific **m, c, and SSE**, representing a line.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Gradient Descent\n",
        "\n",
        "- **Gradient Descent** involves descending down the gradient of the bowl (convex function) to minimize the error.\n",
        "\n",
        "### Formula:\n",
        "The parameters \\(m\\) (slope) and \\(c\\) (intercept) are updated iteratively as follows:  \n",
        "\\[\n",
        "m = m - \\alpha \\frac{\\partial J}{\\partial m}\n",
        "\\]\n",
        "\\[\n",
        "c = c - \\alpha \\frac{\\partial J}{\\partial c}\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- \\(J\\): Cost function (\\(SSE\\))  \n",
        "- \\(\\frac{\\partial J}{\\partial m}\\): Partial derivative of \\(J\\) with respect to \\(m\\)  \n",
        "- \\(\\frac{\\partial J}{\\partial c}\\): Partial derivative of \\(J\\) with respect to \\(c\\)  \n",
        "- \\(\\alpha\\): Learning rate (a small positive value that controls step size)  \n",
        "\n",
        "### >> Gradient Descent:\n",
        "- Step which is drawn with the help of partial derivative by **optimizing error** → **Learning Step**  \n",
        "- The learning step becomes **smaller and smaller** in every iteration.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Key Components of Gradient Descent\n",
        "1. **Partial Derivatives**:  \n",
        "   - Measures the **rate of change** of the error with respect to small changes in \\(m\\) and \\(c\\).  \n",
        "   - Determines how the error increases or decreases with small changes in \\(m\\) and \\(c\\).\n",
        "\n",
        "2. The algorithm uses partial derivatives internally to optimize the values of \\(m\\) and \\(c\\), ensuring that the **error decreases**:  \n",
        "   \\[\n",
        "   \\frac{\\partial J}{\\partial m} = \\frac{-2}{n} \\sum (Y_a - Y_p) x\n",
        "   \\]\n",
        "   \\[\n",
        "   \\frac{\\partial J}{\\partial c} = \\frac{-2}{n} \\sum (Y_a - Y_p)\n",
        "   \\]\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Errors:\n",
        "- **Residual error (SSE/RSS)**:  \n",
        "  \\[\n",
        "  \\text{Residual} = \\text{Actual} - \\text{Predicted}\n",
        "  \\]\n",
        "\n",
        "- **Regression error (SSR)**:  \n",
        "  \\[\n",
        "  \\text{Regression} = \\text{Predicted} - \\text{Mean}\n",
        "  \\]\n",
        "\n",
        "- **Total error (SST)**:  \n",
        "  \\[\n",
        "  \\text{Total} = \\text{Residual} + \\text{Regression} = \\text{Actual} - \\text{Mean}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Coefficient of Determination (\\(R^2\\))\n",
        "\n",
        "- **Purpose**: To evaluate the model's performance and check its reliability.\n",
        "\n",
        "### Key Points:\n",
        "1. **If we have one independent variable**:\n",
        "   - Called the **Coefficient of Determination**.\n",
        "   - Denoted as \\(R \\times R\\).\n",
        "2. **If we have more than one independent variable**:\n",
        "   - Called the **Coefficient of Multiple Determination**.\n",
        "\n",
        "- Indicates how much of the **total variance** in \\(Y\\) is explained by the model.\n",
        "- \\(R^2\\) ranges between **0 and 1**:\n",
        "  - A value closer to **1** indicates higher accuracy.\n",
        "  - \\(R^2 = 1\\): Model explains 100% of variance, \\(SSE = 0\\).\n",
        "  - \\(R^2 = 0\\): Model explains none of the variance.\n",
        "  - \\(R^2 < 0\\): Indicates poor model performance.\n",
        "\n",
        "### Formula:\n",
        "\\[\n",
        "R^2 = 1 - \\frac{SSE}{SST}\n",
        "\\]\n",
        "Where:\n",
        "\\[\n",
        "SSE = \\sum (\\text{Actual} - \\text{Predicted})^2\n",
        "\\]\n",
        "\\[\n",
        "SST = \\sum (\\text{Actual} - \\text{Mean})^2\n",
        "\\]\n",
        "\n",
        "### Example:\n",
        "- Adding variables can either improve or degrade \\(R^2\\):\n",
        "  - Good predictor → \\(R^2\\) increases.\n",
        "  - Poor predictor → \\(R^2\\) slightly decreases.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Adjusted \\(R^2\\)\n",
        "- Adjusted \\(R^2\\) accounts for the number of predictors and penalizes unnecessary complexity.\n",
        "- A better metric when comparing models with varying numbers of predictors.\n"
      ],
      "metadata": {
        "id": "bkXjTa2cU51i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRvTpuZJhI9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vj4GnKZ938cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adjusted R-Squared and Related Concepts**\n",
        "\n",
        "## **1. Adjusted R-Squared (A-R²)**\n",
        "\n",
        "### **What is Adjusted R²?**\n",
        "- A modified version of R² that accounts for the number of independent variables in the model.\n",
        "- Increases only when a good predictor is added.\n",
        "- Always less than or equal to R².\n",
        "\n",
        "### **Formula**  \n",
        "\\[\n",
        "A\\_R² = 1 - \\frac{{(1 - R²) \\cdot (n - 1)}}{{n - k - 1}}\n",
        "\\]  \n",
        "Where:  \n",
        "- \\(n\\): Sample size  \n",
        "- \\(k\\): Number of independent variables  \n",
        "- \\(R²\\): Coefficient of determination  \n",
        "\n",
        "### **Example Calculation**  \n",
        "For \\(n = 100\\), \\(k = 8\\), and \\(R² = 0.93\\):  \n",
        "\\[\n",
        "A\\_R² = 1 - \\frac{{(1 - 0.93) \\cdot (100 - 1)}}{{100 - 8 - 1}} = 0.9238\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Assumptions of Linear Regression**\n",
        "\n",
        "1. **Linearity:**  \n",
        "   The relationship between dependent and independent variables is linear.\n",
        "\n",
        "2. **Independence:**  \n",
        "   Independent variables are not related to each other.\n",
        "\n",
        "3. **Normality of Errors:**  \n",
        "   Errors (residuals) should follow a normal distribution.\n",
        "\n",
        "4. **Homoscedasticity:**  \n",
        "   Errors should have constant variance across all values of predictors.\n",
        "\n",
        "5. **No Multicollinearity:**  \n",
        "   Independent variables should not have high correlation with each other.\n",
        "\n",
        "6. **Mean of Residuals:**  \n",
        "   The mean of residuals should be zero.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Variance Inflation Factor (VIF)**\n",
        "\n",
        "### **What is VIF?**\n",
        "- Detects multicollinearity among independent variables.\n",
        "\n",
        "### **VIF Ranges**\n",
        "- \\(1\\): Not correlated  \n",
        "- \\(1-5\\): Moderately correlated  \n",
        "- \\(>5\\): Highly correlated  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. Ordinary Least Squares (OLS)**\n",
        "\n",
        "### **Definition**\n",
        "- A statistical method to estimate the relationship between variables by minimizing the sum of squared differences between actual and predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "### **What is SGD?**\n",
        "- A faster version of gradient descent, especially for large datasets.  \n",
        "- Uses random sampling instead of the entire dataset for each iteration.\n",
        "\n",
        "### **Why Use SGD?**\n",
        "- Efficient for high-dimensional data.\n",
        "- Faster for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Evaluation Metrics for Regression**\n",
        "\n",
        "### **a) Mean Absolute Error (MAE)**  \n",
        "**Formula:**  \n",
        "\\[\n",
        "MAE = \\frac{{\\sum |y_{\\text{actual}} - y_{\\text{predicted}}|}}{n}\n",
        "\\]  \n",
        "**Key Points:**  \n",
        "- Robust to outliers.  \n",
        "- Optimal predictions are near the median of target values.\n",
        "\n",
        "---\n",
        "\n",
        "### **b) Mean Squared Error (MSE)**  \n",
        "**Formula:**  \n",
        "\\[\n",
        "MSE = \\frac{{\\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2}}{n}\n",
        "\\]  \n",
        "**Key Points:**  \n",
        "- Penalizes larger errors.  \n",
        "- Sensitive to outliers.\n",
        "\n",
        "---\n",
        "\n",
        "### **c) Root Mean Squared Error (RMSE)**  \n",
        "**Formula:**  \n",
        "\\[\n",
        "RMSE = \\sqrt{\\frac{{\\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2}}{n}}\n",
        "\\]  \n",
        "**Key Points:**  \n",
        "- Commonly used in deep learning.  \n",
        "- Sensitive to outliers.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Advantages and Disadvantages of Linear Regression**\n",
        "\n",
        "### **Advantages**\n",
        "1. Easy to implement and interpret.  \n",
        "2. Works well when assumptions hold true.  \n",
        "3. Scaling does not affect the model.  \n",
        "4. Useful for dimensionality reduction.\n",
        "\n",
        "### **Disadvantages**\n",
        "1. Sensitive to outliers.  \n",
        "2. Assumes linear relationships between variables.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vl2RBD4Y4mIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1: Regression Based\n"
      ],
      "metadata": {
        "id": "HxMRP-H57QJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I75-ZWpZ568D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris Dataset Sepal Length Prediction\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project involves predicting the **Sepal Length** of the Iris dataset based on the other features, including **Sepal Width**, **Petal Length**, **Petal Width**, and **Species**. We will implement a simple machine learning model, focusing on data exploration, preprocessing, visualization, and model evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Problem Statement\n",
        "\n",
        "The task is to predict the **SepalLengthCm** using the other available features:\n",
        "- **Independent Variables:** SepalWidthCm, PetalWidthCm, PetalLengthCm, Species\n",
        "- **Dependent Variable:** SepalLengthCm\n",
        "\n",
        "### Business Problem:\n",
        "Predict `SepalLengthCm` based on various measurements in the Iris dataset to build a model that can provide insights into how these measurements impact sepal length.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Gathering\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np  # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import seaborn as sns  # For advanced visualization\n",
        "\n",
        "# Display font caching message\n",
        "print(\"Matplotlib is building the font cache; this may take a moment.\")\n",
        "We begin by importing the libraries necessary for the project. Pandas will be used for data manipulation, NumPy for numerical operations, and Matplotlib and Seaborn for visualization.\n",
        "\n",
        "Next, we load the Iris dataset into a Pandas DataFrame.\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "df = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Display first 5 rows of the dataset\n",
        "print(df.head())\n",
        "Here, the first few rows of the dataset are displayed to get an overview of the data.\n",
        "\n",
        "Checking Basic Information\n",
        "\n",
        "# Check the shape (rows and columns)\n",
        "row_count = df.shape[0]\n",
        "col_count = df.shape[1]\n",
        "print(f\"Row count: {row_count}, Column count: {col_count}\")\n",
        "\n",
        "# Display column names\n",
        "print(df.columns)\n",
        "\n",
        "# Display dataset information\n",
        "print(df.info())\n",
        "\n",
        "# Check for null values\n",
        "print(df.isnull().sum())\n",
        "We check the number of rows and columns, the column names, and the overall structure of the dataset. We also check for any missing values.\n",
        "\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "3.1 Analyze the Id Column\n",
        "\n",
        "# Display unique values in the 'Id' column\n",
        "print(df['Id'].unique())\n",
        "\n",
        "# Count unique values in 'Id'\n",
        "print(df['Id'].nunique())\n",
        "\n",
        "# Drop the 'Id' column\n",
        "df.drop('Id', axis=1, inplace=True)\n",
        "The Id column is unnecessary for modeling, so we examine its unique values and drop it from the dataset.\n",
        "\n",
        "3.2 Analyze the SepalWidthCm Column\n",
        "\n",
        "# Count unique values in 'SepalWidthCm'\n",
        "print(df['SepalWidthCm'].nunique())\n",
        "\n",
        "# Visualize the distribution of SepalWidthCm\n",
        "sns.histplot(df['SepalWidthCm'])\n",
        "plt.show()\n",
        "\n",
        "sns.distplot(df['SepalWidthCm'])\n",
        "plt.show()\n",
        "We investigate the SepalWidthCm column, look for unique values, and visualize its distribution using histograms. The Seaborn and Matplotlib libraries help visualize the data effectively.\n",
        "\n",
        "3.3 Correlation Heatmap\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "We calculate the correlation between different features and visualize the correlation matrix using a heatmap to understand the relationships between variables.\n",
        "\n",
        "4. Data Preprocessing\n",
        "4.1 Handle Missing Values\n",
        "We ensure that there are no missing values in the dataset.\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "If there were any missing values, we would handle them by either filling or dropping rows/columns. In this case, let's assume no missing values are found.\n",
        "\n",
        "4.2 Encoding Categorical Variables\n",
        "Since the Species column is categorical, we will encode it into numeric values.\n",
        "\n",
        "\n",
        "# Encode the 'Species' column\n",
        "df['Species'] = df['Species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n",
        "The Species column is mapped to numerical values using the map() function.\n",
        "\n",
        "5. Model Building\n",
        "5.1 Train-Test Split\n",
        "We split the data into training and testing sets to evaluate the model performance later.\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X = df.drop('SepalLengthCm', axis=1)  # Features\n",
        "y = df['SepalLengthCm']  # Target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "5.2 Model Selection\n",
        "We'll use a simple linear regression model to predict SepalLengthCm.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "5.3 Model Evaluation\n",
        "After training the model, we evaluate it using the testing set and calculate performance metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predict the target variable on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n",
        "The metrics are printed to check how well the model performs.\n",
        "\n",
        "6. Model Interpretation and Visualization\n",
        "We can visualize the true vs. predicted values for a better understanding of the model’s performance.\n",
        "\n",
        "\n",
        "# Scatter plot to visualize true vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('True vs Predicted Sepal Length')\n",
        "plt.show()\n",
        "The scatter plot shows the relationship between the true and predicted values. A red line represents perfect predictions, where the true values match the predicted ones.\n",
        "\n",
        "7. Conclusion\n",
        "The linear regression model successfully predicts the Sepal Length of the Iris dataset.\n",
        "The model evaluation metrics indicate the prediction quality.\n",
        "Future improvements could involve using more complex models like decision trees or neural networks for better accuracy.\n",
        "Future Work\n",
        "Experiment with other models like Random Forest or XGBoost.\n",
        "Explore hyperparameter tuning to optimize model performance.\n",
        "Extend the analysis to predict other features like PetalLengthCm or PetalWidthCm."
      ],
      "metadata": {
        "id": "3N3lExFXVCG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jd4lxjkVCNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuY23z79VCS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJYI5m1rhJAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Regression Based\n"
      ],
      "metadata": {
        "id": "E6Ofsi7mC3Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Car Price Prediction\n",
        "\n",
        "This project aims to predict the price of cars using various attributes like dimensions, engine characteristics, fuel efficiency, and other features. Below is the detailed explanation of the steps involved in this project.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Problem Statement\n",
        "The objective is to build a regression model that predicts the price of cars based on the given dataset. The dataset contains various features of cars such as engine specifications, dimensions, and performance metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Gathering\n",
        "The data for this project is loaded from a CSV file named `autos_dataset.csv`.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('autos_dataset.csv')\n",
        "df.head().T  # Transpose to view features clearly\n"
      ],
      "metadata": {
        "id": "HJDviouVCzf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "3. Loading the Dataset\n",
        "\n",
        "# Load the Iris dataset into a pandas DataFrame\n",
        "df = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to inspect it\n",
        "print(df.head())  # This will show the first five rows of the dataset\n",
        "The dataset is loaded into a pandas DataFrame. The first few rows are displayed to inspect the data and get an understanding of its structure.\n",
        "\n",
        "4. Data Exploration\n",
        "4.1 Dataset Shape\n",
        "\n",
        "# Checking the shape of the dataset (number of rows and columns)\n",
        "row_count = df.shape[0]  # Number of rows\n",
        "col_count = df.shape[1]  # Number of columns\n",
        "print(f\"Row count: {row_count}, Column count: {col_count}\")  # Output the shape of the dataset\n",
        "The shape of the dataset is checked to understand how many rows and columns are present.\n",
        "\n",
        "4.2 Column Names\n",
        "\n",
        "# Display the column names to understand the structure\n",
        "print(df.columns)\n",
        "We display the column names to understand the structure of the dataset.\n",
        "\n",
        "4.3 General Information\n",
        "\n",
        "# Check general information about the dataset, like data types and non-null counts\n",
        "print(df.info())\n",
        "We inspect the general information about the dataset, including data types and missing values.\n",
        "\n",
        "4.4 Missing Values\n",
        "\n",
        "# Check for missing values in each column\n",
        "print(df.isnull().sum())  # This will show the count of null values for each column\n",
        "We check for missing values in the dataset, which might need handling during data preprocessing.\n",
        "\n",
        "4.5 Unique Values in 'Id'\n",
        "\n",
        "# Analyze the 'Id' column, as it may not be useful for prediction\n",
        "print(df['Id'].unique())  # List all unique values in the 'Id' column\n",
        "print(df['Id'].nunique())  # Count the number of unique values in 'Id'\n",
        "We analyze the 'Id' column to check if it is necessary for prediction. If it doesn't contribute to prediction, it will be dropped.\n",
        "\n",
        "5. Data Cleaning\n",
        "5.1 Drop 'Id' Column\n",
        "\n",
        "# Drop the 'Id' column as it doesn't contribute to predicting the Sepal Length\n",
        "df.drop('Id', axis=1, inplace=True)  # axis=1 specifies that we are dropping a column, not a row\n",
        "The 'Id' column is dropped as it does not provide useful information for prediction.\n",
        "\n",
        "5.2 Visualizing 'SepalWidthCm'\n",
        "\n",
        "# Visualize the distribution of 'SepalWidthCm' using histograms\n",
        "sns.histplot(df['SepalWidthCm'])  # Visualize the distribution using Seaborn\n",
        "plt.show()  # Show the plot\n",
        "We visualize the distribution of the 'SepalWidthCm' feature using histograms to understand its distribution.\n",
        "\n",
        "6. Data Preprocessing\n",
        "6.1 Encoding 'Species' Column\n",
        "\n",
        "# Encode the categorical 'Species' column into numerical format using pd.get_dummies\n",
        "# This creates binary columns for each category in the 'Species' column\n",
        "df = pd.get_dummies(df, columns=['Species'], drop_first=True)\n",
        "\n",
        "# Display the updated dataframe to ensure that the 'Species' column has been encoded correctly\n",
        "print(df.head())  # Display first few rows of the updated dataframe\n",
        "The categorical 'Species' column is encoded into numerical format using one-hot encoding.\n",
        "\n",
        "7. Model Training and Evaluation\n",
        "7.1 Splitting the Dataset\n",
        "\n",
        "# Now, we split the dataset into features (X) and target variable (y)\n",
        "X = df.drop('SepalLengthCm', axis=1)  # Features: Drop the target variable\n",
        "y = df['SepalLengthCm']  # Target: SepalLengthCm column\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "from sklearn.model_selection import train_test_split  # Importing the train_test_split function\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% training and 20% testing\n",
        "We split the dataset into features (X) and the target variable (y) and further split it into training and testing sets.\n",
        "\n",
        "7.2 Initializing and Training the Model\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "from sklearn.linear_model import LinearRegression  # Importing the LinearRegression model\n",
        "model = LinearRegression()  # Creating an instance of the linear regression model\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, y_train)  # Fit the model to the training data\n",
        "We initialize and train a Linear Regression model using the training data.\n",
        "\n",
        "7.3 Making Predictions\n",
        "\n",
        "# Use the trained model to make predictions on the test set\n",
        "y_pred = model.predict(X_test)  # Predict the Sepal Length on the test set\n",
        "We use the trained model to predict the Sepal Length on the test set.\n",
        "\n",
        "8. Model Evaluation\n",
        "8.1 Performance Metrics\n",
        "\n",
        "# Evaluate the performance of the model using Mean Squared Error and R-squared score\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # Importing the metrics for evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)  # Calculate Mean Squared Error (MSE)\n",
        "r2 = r2_score(y_test, y_pred)  # Calculate R-squared score\n",
        "\n",
        "# Output the evaluation metrics to check the model's performance\n",
        "print(f\"Mean Squared Error: {mse}\")  # Display MSE\n",
        "print(f\"R-squared Score: {r2}\")  # Display R-squared score\n",
        "We evaluate the performance of the model using Mean Squared Error (MSE) and R-squared score.\n",
        "\n",
        "9. Model Coefficients\n",
        "\n",
        "# Finally, display the coefficients and intercept of the trained linear regression model\n",
        "print(\"Coefficients:\", model.coef_)  # Model's coefficients (weight for each feature)\n",
        "print(\"Intercept:\", model.intercept_)  # Model's intercept (constant term)\n",
        "We display the model's coefficients and intercept to understand the influence of each feature on the prediction.\n",
        "\n",
        "Conclusion\n",
        "The model has been trained and evaluated. The Mean Squared Error and R-squared Score provide insight into the model's accuracy. The coefficients tell us how much each feature influences the prediction of Sepal Length."
      ],
      "metadata": {
        "id": "Gy12oDmwhJCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6LPINplhJEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CP3YkdiNhJG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcbWuNwHhJJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xeGwWeCZhJL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhV_Ia3QhJOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUgLTXgmhJQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1SMt0n9hJTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYdlUe_3hJVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84GYoPoshJZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jFYEH9JhJbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDfRHGwhhLR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}